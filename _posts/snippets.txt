AIC (Akaike Information Criterion), BIC (Bayesian Information Criterion), and HQIC (Hannan-Quinn Information Criterion) are all model selection criteria used to compare and select the best model among a set of candidate models. They assess the goodness of fit while penalizing model complexity to avoid overfitting. The main differences lie in how they penalize the number of parameters in the model.

1. AIC (Akaike Information Criterion):
   - Formula: AIC = 2k - 2ln(L), where k is the number of parameters and L is the maximum likelihood estimate.
   - AIC penalizes the number of parameters less strongly compared to BIC and HQIC.
   - It tends to favor more complex models and may select models with a higher number of parameters.
   - AIC is asymptotically efficient, meaning it selects the model that minimizes the mean squared error of prediction as the sample size approaches infinity.

2. BIC (Bayesian Information Criterion):
   - Formula: BIC = k * ln(n) - 2ln(L), where k is the number of parameters, n is the sample size, and L is the maximum likelihood estimate.
   - BIC penalizes the number of parameters more strongly than AIC, especially for larger sample sizes.
   - It tends to favor simpler models and may select models with fewer parameters compared to AIC.
   - BIC is consistent, meaning it selects the true model with probability approaching 1 as the sample size approaches infinity, assuming the true model is among the candidate models.

3. HQIC (Hannan-Quinn Information Criterion):
   - Formula: HQIC = 2k * ln(ln(n)) - 2ln(L), where k is the number of parameters, n is the sample size, and L is the maximum likelihood estimate.
   - HQIC lies between AIC and BIC in terms of penalizing model complexity.
   - It balances the strengths of AIC and BIC, providing a middle ground between efficiency and consistency.
   - HQIC is consistent, similar to BIC, but with a slower rate of convergence.

When to use each criterion:
- AIC is preferred when the goal is to minimize the prediction error and select a model with good predictive performance. It is useful when the true model is complex or when the sample size is relatively small compared to the number of parameters.
- BIC is preferred when the goal is to identify the true model structure and when the sample size is large. It is useful when the true model is simple and the focus is on model interpretation rather than prediction.
- HQIC can be used as a compromise between AIC and BIC when the sample size is moderate and there is uncertainty about the complexity of the true model.

In practice, it is common to calculate all three criteria and compare the results. If the criteria agree on the best model, that model can be selected with more confidence. If they disagree, it may be worth considering the specific goals of the analysis (prediction vs. interpretation) and the sample size to make a decision.

It's important to note that these criteria are relative measures and should be used to compare models within the same set of candidate models. Lower values of AIC, BIC, and HQIC indicate better models.

---

A unit root in the context of time series analysis refers to a characteristic of a stochastic process, indicating that the process is non-stationary. Specifically, a time series with a unit root contains a stochastic trend, meaning that shocks to the series have a permanent effect on its future values. This is in contrast to stationary processes where shocks only have a temporary effect.

### Definition

A time series \( Y_t \) has a unit root if it can be described by a model such as:

\[ Y_t = \rho Y_{t-1} + \epsilon_t \]

where \( \epsilon_t \) is a white noise error term. If \( \rho = 1 \), then \( Y_t \) has a unit root. In this case, the process is typically referred to as a random walk. If \( |\rho| < 1 \), the process is stationary, and if \( \rho > 1 \), the process is explosive.

### Implications of a Unit Root

1. **Non-Stationarity**: A time series with a unit root is non-stationary. This means it has a time-dependent structure, particularly in its mean and variance. For instance, a random walk, which is a common example of a unit root process, has a variance that grows over time.

2. **Statistical Properties**: The presence of a unit root affects the statistical properties of the series. For example, standard hypothesis tests (like t-tests and F-tests) that assume stationarity will no longer have valid standard distributions, leading to unreliable statistical inference.

3. **Forecasting**: Predictions from a unit root process have increasing uncertainty as the forecast horizon extends, due to the accumulating effect of new shocks.

### Testing for Unit Roots

Several tests are used to determine whether a time series has a unit root, implying non-stationarity:

1. **ADF (Augmented Dickey-Fuller) Test**: This test checks the null hypothesis that a unit root is present against the alternative that the series is stationary. It involves estimating the parameter \( \rho \) in the model \( \Delta Y_t = (\rho - 1) Y_{t-1} + \epsilon_t \) and testing if it is significantly different from zero.

2. **Phillips-Perron (PP) Test**: Similar to the ADF test but adjusts for serial correlation and heteroskedasticity in the error term without adding lagged difference terms.

3. **KPSS (Kwiatkowski-Phillips-Schmidt-Shin) Test**: This test has a null hypothesis of stationarity, opposite to that of the ADF and PP tests. It examines the stationarity of the series around a deterministic trend.

### Dealing with Unit Roots

If a unit root is present, analysts often difference the series—subtract the previous value from the current value—to achieve stationarity. This transformation is critical for making the series suitable for many standard time series analysis techniques and forecasting models, which assume or require stationarity.